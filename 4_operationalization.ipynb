{"cells":[{"cell_type":"markdown","source":["# Step 4: Model operationalization & Deployment\n\nIn this script, we load the model from the `Code/3_model_building.ipynb` Jupyter notebook and the labeled feature data set constructed in the `Code/2_feature_engineering.ipynb` notebook in order to build the model deployment artifacts. \n\n\nThe remainder of this notebook details steps required to deploy and operationalize the model using Azure Machine Learning service SDK."],"metadata":{}},{"cell_type":"code","source":["## setup our environment by importing required libraries\nimport json\nimport os\nimport shutil\nimport time\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\n\n# for creating pipelines and model\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n\n# setup the pyspark environment\nfrom pyspark.sql import SparkSession\n\n# AML SDK libraries\nfrom azureml.core import Workspace, Run\nfrom azureml.core.model import Model\nfrom azureml.core.image import ContainerImage\nfrom azureml.core.conda_dependencies import CondaDependencies \nfrom azureml.core.webservice import AciWebservice,Webservice\n\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["We need to load the feature data set from memory to construct the operationalization schema."],"metadata":{}},{"cell_type":"code","source":["features_file = 'featureengineering_files.parquet'\ntarget_dir = \"dbfs:/dataset/\"\n\nfeat_data = spark.read.parquet(os.path.join(target_dir,features_file))\nfeat_data.limit(5).toPandas().head(5)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["The steps that makes up model operationalization are:\n\n- The model you trained in notebook 3_model_building\n- A scoring script to show how to use the model\n- Conda yml file containing packages need to be installed\n- A configuration definition object to build the ACI\n\n\nFor more details, go to https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-model-management-and-deployment"],"metadata":{}},{"cell_type":"markdown","source":["Here, we copy the model saved in notebook 03_building_model to local directory"],"metadata":{}},{"cell_type":"code","source":["model_name = 'pdmrfull.model'\nmodel_local = \"file:\" + os.getcwd() + \"/\" + model_name\nmodel_dir = os.path.join(\"dbfs:/model/\", model_name)\ndbutils.fs.cp(model_dir, model_local, True)\ndisplay(dbutils.fs.ls(model_local))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["We register the model in the experiment in Azure Machine learning service"],"metadata":{}},{"cell_type":"code","source":["ws = Workspace.from_config()\nmodel_name = 'pdmrfull.model'\nmodel = Model.register(model_path= model_name, model_name=model_name , workspace=ws)\nprint(\"Registered:\", model.name)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Here we define the conda dependencies used by scoring script."],"metadata":{}},{"cell_type":"code","source":["conda_env = CondaDependencies.create(conda_packages=['pyspark'])\nwith open(\"conda_env.yml\",\"w\") as f:\n    f.write(conda_env.serialize_to_string())"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Here we define the scoring script that will be backed into docker image for prediction serving"],"metadata":{}},{"cell_type":"code","source":["%%writefile score.py\n\nfrom azureml.core.model import Model\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\nfrom pyspark.ml import PipelineModel\nimport pyspark\nimport json\n\ndef init():\n    \n    global pipeline,spark\n        \n    spark = pyspark.sql.SparkSession.builder.appName(\"Predictive maintenance service\").getOrCreate()\n    model_path = Model.get_model_path('pdmrfull.model')\n    pipeline = PipelineModel.load(model_path)\n    \n\ndef run(raw_data):\n    \n    try:\n        sc = spark.sparkContext\n        input_list = json.loads(raw_data)\n        input_rdd = sc.parallelize(input_list)\n        input_df = spark.read.json(input_rdd)\n        \n        key_cols =['label_e','machineID','dt_truncated', 'failure','model_encoded','model']\n        input_features = input_df.columns\n        \n        # Remove unseen features by the model during training\n        input_features = [x for x in input_features if x not in set(key_cols)]\n        \n        \n        va = VectorAssembler(inputCols=(input_features), outputCol='features')\n        data = va.transform(input_df).select('machineID','features')\n        score = pipeline.transform(data)\n        predictions = score.collect()\n        \n        preds = [str(x['prediction']) for x in predictions]\n        result = preds\n    except Exception as e:\n        result = str(e)\n        \n    return json.dumps({\"result\":result})"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["###  Provision ACI for image deployment\n\nNote that this may take a couple of minutes for ACI deployment to complete"],"metadata":{}},{"cell_type":"code","source":["image_config = ContainerImage.image_configuration(runtime= \"spark-py\",\n                                 execution_script=\"score.py\",\n                                 conda_file=\"conda_env.yml\")\n\naci_config = AciWebservice.deploy_configuration(cpu_cores = 2, \n                                               memory_gb = 4, \n                                               tags = {'type': \"predictive_maintenance\"}, \n                                               description = \"Predictive maintenance classifier\")\n\n\n\naci_service_name = 'pred-maintenance-service'\nprint(aci_service_name)\n\naci_service = Webservice.deploy_from_model(workspace=ws, \n                                        name=aci_service_name,\n                                        deployment_config = aci_config,\n                                        models = [model],\n                                        image_config = image_config\n                                          )\n\n\naci_service.wait_for_deployment(True)\nprint(aci_service.state)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### Invoke web service endpoint for prediction\n\nFirst we get a sample test observation that we can score. For this, we can randomly select a single record from the test data.."],"metadata":{}},{"cell_type":"code","source":["test_sample = (feat_data.sample(False, .8).limit(1))\nexcluded_cols = {'label_e','machineID','dt_truncated','failure','model_encoded','model'}\ninput_features = set(test_sample.columns)- excluded_cols\n\n\nraw_input = test_sample.toJSON().collect()\nprediction = aci_service.run(json.dumps(raw_input))\n\nprint(prediction)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Delete ACI service to free up resources"],"metadata":{}},{"cell_type":"code","source":["aci_service.delete()"],"metadata":{},"outputs":[],"execution_count":19}],"metadata":{"kernelspec":{"display_name":"PredictiveMaintenance dlvmjme","language":"python","name":"predictivemaintenance_dlvmjme"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":"3"},"version":"3.5.2","nbconvert_exporter":"python","file_extension":".py"},"name":"4_operationalization","notebookId":3296429392325481},"nbformat":4,"nbformat_minor":0}
